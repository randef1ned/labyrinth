<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="labyrinth">
<title>Introduction and inspirations of labyrinth • labyrinth</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.11/clipboard.min.js" integrity="sha512-7O5pXpc0oCRrxk8RUfDYFgn0nO1t+jLuIOQdOMRp4APB7uZ4vSjspzp5y6YDtDs4VzUSTbWzBFZ/LKJhnyFOKw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introduction and inspirations of labyrinth">
<meta property="og:description" content="labyrinth">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light" data-bs-theme="light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">labyrinth</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/labyrinth.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/preface.html">Introduction and inspirations of labyrinth</a>
    <a class="dropdown-item" href="../articles/training.html">Training the model</a>
  </div>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/randef1ned/labyrinth/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Introduction and inspirations of labyrinth</h1>
                        <h4 data-toc-skip class="author">Yinchun Su</h4>
            
      
      <small class="dont-index">Source: <a href="https://github.com/randef1ned/labyrinth/blob/HEAD/vignettes/preface.Rmd" class="external-link"><code>vignettes/preface.Rmd</code></a></small>
      <div class="d-none name"><code>preface.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="from-human-memory-to-activation-diffusion-network">1. From human memory to activation diffusion network<a class="anchor" aria-label="anchor" href="#from-human-memory-to-activation-diffusion-network"></a>
</h2>
<p>Memory is a fundamental cognitive process that allows the brain to
store, acquire, and recall information. It serves as a temporary storage
system when sensory cues disappear <span class="citation">(<a href="#ref-benjamin_memory_2007">Benjamin 2007</a>)</span>. Memory plays
a crucial role in encoding, storing, retaining, and recalling everything
from simple sensory data to complex knowledge and experiences.
Additionally, memory is the basis for learning, planning, and
decision-making <span class="citation">(<a href="#ref-benjamin_memory_2007">Benjamin 2007</a>; <a href="#ref-nussenbaum_memorys_2020">Nussenbaum, Prentis, and Hartley
2020</a>)</span>. Specifically, it enables us to learn from past
experiences and simulate potential future outcomes, thereby influencing
current behavior and future actions <span class="citation">(<a href="#ref-schacter_future_2012">Schacter et al. 2012</a>)</span>.</p>
<p>The formation of memories, their recall, and reasoning based on them
involve a combination of systems and physiological processes that allow
humans to adapt well to their environment <span class="citation">(<a href="#ref-schacter_future_2012">Schacter et al. 2012</a>; <a href="#ref-camina_neuroanatomical_2017">Camina and Güell 2017</a>; <a href="#ref-nairne_adaptive_2016">Nairne and Pandeirada
2016</a>)</span>.Memory formation comprises three stages: information
perception, encoding, and storage <span class="citation">(<a href="#ref-atkinson_human_1968">Atkinson and Shiffrin 1968</a>)</span>.
These stages correspond to three types of memory: (1) sensory memory
<span class="citation">(<a href="#ref-atkinson_human_1968">Atkinson and
Shiffrin 1968</a>)</span> briefly stores raw physical stimuli from
primary receptors such as vision and hearing; (2) short-term memory
(STM) <span class="citation">(<a href="#ref-baddeley_episodic_2000">Baddeley 2000</a>)</span> involves
the transient storage and manipulation of information, allowing
individuals to temporarily memorize small amounts of data to perform
current and future tasks; (3) long-term memory (LTM) <span class="citation">(<a href="#ref-camina_neuroanatomical_2017">Camina and
Güell 2017</a>)</span> is the long-term storage of information, divided
into episodic and implicit memory. Episodic memory consists of knowledge
processed and recalled at a conscious level, such as personal
experiences and specialized knowledge, while implicit memory encompasses
skills and habits expressed without conscious awareness, such as fear,
riding a bicycle, heart rate regulation, and other conditioned reflexes
<span class="citation">(<a href="#ref-smith_multiple_2008">Smith and
Grossman 2008</a>)</span>. In contrast to the limited-capacity sensory
memory and STM, LTM is a more complex cognitive system with an unlimited
capacity for long-term storage and retrieval of a wide range of
information, including factual knowledge and personal experiences.</p>
<p>Memory can also be categorized into contextual memory, referring to
an individual’s personal experiences, and semantic memory, referring to
textual knowledge about concepts <span class="citation">(<a href="#ref-renoult_knowing_2019">Renoult et al. 2019</a>)</span>.
Storing contextual and semantic knowledge allows individuals to
construct new knowledge based on past experiences, facilitating their
survival <span class="citation">(<a href="#ref-kazanas_survival_2015">Kazanas and Altarriba
2015</a>)</span>. In addition to storing information and memories, LTM
plays an important role in learning and reasoning. It can automatically
relate relationships and attributes between objects <span class="citation">(<a href="#ref-nairne_adaptive_2016">Nairne and
Pandeirada 2016</a>)</span>, giving individuals the ability to use
stored skills and concepts to make rational decisions by evaluating
different choices in various environments and predicting possible
outcomes <span class="citation">(<a href="#ref-camina_neuroanatomical_2017">Camina and Güell
2017</a>)</span>.</p>
<p>The formation and consolidation of LTM involve several brain regions,
including the prefrontal lobe, associated with working memory and
situational memory in LTM <span class="citation">(<a href="#ref-blumenfeld_lateral_2019">Blumenfeld and Ranganath
2019</a>)</span>, and the temporal lobe, associated with semantic memory
in LTM <span class="citation">(<a href="#ref-simmons_anterior_2009">Simmons and Martin 2009</a>)</span>.
The hippocampus acts as a relay station for information <span class="citation">(<a href="#ref-squire_medial_1991">Squire and
Zola-Morgan 1991</a>)</span> and can integrate situational memory into
the semantic memory knowledge network stored in LTM <span class="citation">(<a href="#ref-renoult_knowing_2019">Renoult et al.
2019</a>)</span>. Consequently, even for the same concept or knowledge,
the knowledge network formed by different individuals can vary.</p>
<p>Each individual has unique experiences and backgrounds, leading to
different understandings and reactions when interpreting the same
information. LTM is stored as a vast and complex semantic network, which
includes various types of interconnected nodes, such as concepts,
memories, and experiences <span class="citation">(<a href="#ref-collins_spreading_activation_1975">Collins and Loftus
1975</a>)</span>. Other kinds of memories or experiences are also
integrated into this network; for example, an individual’s
representation of abstract concepts (e.g., time) may be based on
physical sensations <span class="citation">(<a href="#ref-casasanto_time_2008">Casasanto and Boroditsky
2008</a>)</span>. In such cases, individuals associate time with their
experiences, forming their knowledge network. This form of organization
is named semantic networks or knowledge networks, emphasizing how
information is interconnected and organized according to meaning <span class="citation">(<a href="#ref-lehmann_semantic_1992">Lehmann
1992</a>)</span>.</p>
<p>In this article, we use the term <strong>semantic network</strong> to
represent the form of memory storage and organization in LTM, while
<strong>knowledge network</strong> refers to an artificially built
knowledge network. In a semantic network, concepts are represented as
nodes, and concept-to-concept relationships are represented as edges
between nodes, with edge weights indicating the strength of the
association. A higher edge weight implies a closer relationship between
two nodes, typically resulting in a higher recall rate after receiving a
stimulus <span class="citation">(<a href="#ref-anderson_spreading_1983">Anderson 1983</a>)</span>. Learning
and memorizing new knowledge and experiences involve building new edges
or reinforcing existing ones. This organization facilitates information
retrieval by enabling individuals to jump from one concept to another in
the network and simultaneously activate neighboring nodes to form
knowledge connections, even if there is no direct correlation between
them <span class="citation">(<a href="#ref-lehmann_semantic_1992">Lehmann 1992</a>)</span>. An
interesting example is that in the semantic network of some police
officers, black people produce a strong association with weapons, and
this association is even stronger if the police officer is
sleep-deprived <span class="citation">(<a href="#ref-james_stability_2018">James 2018</a>)</span>. Another example
is that an individual’s preference for Coca-Cola or McDonald’s is
determined by their attitude and reflected in their semantic network
<span class="citation">(<a href="#ref-lee_comparison_2013">Lee and Kim
2013</a>; <a href="#ref-karpinski_attitude_2005">Karpinski, Steinman,
and Hilton 2005</a>)</span>.</p>
<p>Homogeneous networks consist of nodes with similar properties or
characteristics <span class="citation">(<a href="#ref-mhatre_homogeneous_2004">Mhatre and Rosenberg
2004</a>)</span>. Nodes represented as the same kind of elements and
edges connected nodes with high correlations, which jointly creating a
homogeneous network. The two examples mentioned above illustrate that
different individuals form different LTMs, and the memory contents
stored in their LTMs do not satisfy homogeneity.</p>
<p>Moving from one concept to an unrelated concept is impossible in a
homogeneous network. The process by which individuals store their
memories in the same semantic network and retrieve information from LTM
is often described as spreading activation, and this network is also
called the spreading activation network <span class="citation">(<a href="#ref-sharifian_hierarchical_1997">Sharifian and Samani
1997</a>)</span>. In this network model, if an initial node is
activated, this activation state spreads to other nodes along connected
edges. This diffusion process can quickly span multiple network layers,
extensively activating the concepts and memories associated with the
initial node. When a node receives activation above a certain threshold,
it is fully activated like neurons. Otherwise, it will not be activated.
This may lead to the recall of specific memories, the formation of
decisions, or the generation of problem-solving strategies.</p>
<p>As mentioned earlier, some unrelated concepts in a semantic network
may have relatively strong associations. The implicit association test
(IAT) paradigm proposed by Greenwald can effectively test the edge
connections between nodes of an individual in a semantic network <span class="citation">(<a href="#ref-greenwald_measuring_1998">Greenwald,
McGhee, and Schwartz 1998</a>; <a href="#ref-greenwald_understanding_2009">Greenwald et al.
2009</a>)</span>. This paradigm tests the strength of association in the
human brain between two nodes, i.e., the edge weights. The mechanisms of
association and activation in activation diffusion networks depend on
the strength of association between nodes. If the strength is high, the
probability of activation is relatively high; if the strength is low,
there is a higher probability of non-activation. This theory partly
explains the forgetting phenomenon that occurs in human memory.
Additionally, activation diffusion networks enable individuals to
retrieve necessary information, reorganize their memories, and apply
knowledge to the same or different situations. In summary, activation
diffusion networks effectively account for the dynamic nature of memory
retrieval and use.</p>
</div>
<div class="section level2">
<h2 id="unique-advantages-of-human-cognitive-abilities">2. Unique advantages of human cognitive abilities<a class="anchor" aria-label="anchor" href="#unique-advantages-of-human-cognitive-abilities"></a>
</h2>
<p>Compared to computer programs, humans possess an ability to think
about problems from different perspectives and exhibit greater
flexibility in knowledge association <span class="citation">(<a href="#ref-lehmann_semantic_1992">Lehmann 1992</a>)</span>. Therefore,
humans have the advantage of applying knowledge from one domain to
another seemingly unrelated domain. For example, concepts from biology
can be transferred to economics <span class="citation">(<a href="#ref-lawlor_mendelian_2008">Lawlor et al. 2008</a>)</span>,
economic models to the field of electronic information <span class="citation">(<a href="#ref-han_rational_2019">Han et al.
2019</a>)</span>, and linguistic concepts to neuroscience <span class="citation">(<a href="#ref-mayberry_neurolinguistic_2018">Mayberry
et al. 2018</a>)</span> and computer science <span class="citation">(<a href="#ref-zhang_algorithm_2023">H. Zhang et al. 2023</a>)</span>. This
characteristic has led humans to create many cross-disciplinary fields,
such as artificial intelligence, computational biology, neuropolitics,
and bioinformatics. Humans can use intuition and creative thinking to
solve problems, and this ability to think across domains allows them to
make new connections between different areas, thereby building new
disciplinary knowledge.</p>
<p>The human brain contains approximately 100 billion neurons and
roughly the same number of glial cells <span class="citation">(<a href="#ref-von_bartheld_search_2016">Bartheld, Bahney, and
Herculano-Houzel 2016</a>)</span>, of each connected to thousands of
others via synapses <span class="citation">(<a href="#ref-herculano_houzel_human_2009">Herculano-Houzel
2009</a>)</span>. Neurons and glial cells form extremely complex
networks. Neurons communicate via the all-or-none principle <span class="citation">(<a href="#ref-pareti_all_or_none_2007">Pareti
2007</a>)</span>, and glial cells play crucial roles in central nervous
system formation, neuronal differentiation, synapse formation <span class="citation">(<a href="#ref-allen_glia_2018">Allen and Lyons
2018</a>)</span>, regulation of neuroinflammatory immunity <span class="citation">(<a href="#ref-yang_neuroinflammation_2019">Yang and
Zhou 2019</a>)</span>, and neurological diseases like dementia <span class="citation">(<a href="#ref-kim_neuron_glia_2020">Kim, Choi, and
Yoon 2020</a>)</span>, in addition to their traditional supportive
functions <span class="citation">(<a href="#ref-wolosker_d_amino_2008">Wolosker et al. 2008</a>)</span>. Such
complexity lays the foundation for an individual’s ability to process
information, experience emotions, maintain awareness, and exhibit
creativity.</p>
<p>Drawing on the fundamentals of human cognition, artificial neural
networks have been simulated using computers to mimic the brain’s
information processing. They emulate human cognitive abilities to some
extent, excelling in tasks like learning, decision-making, and pattern
recognition that humans are naturally proficient at <span class="citation">(<a href="#ref-agatonovic_kustrin_basic_2000">Agatonovic-Kustrin and
Beresford 2000</a>; <a href="#ref-parisi_artificial_1997">Parisi
1997</a>)</span>. The simulation of human cognitive abilities has shown
great potential <span class="citation">(<a href="#ref-parisi_artificial_1997">Parisi 1997</a>; <a href="#ref-zahedi_introduction_1991">Zahedi 1991</a>)</span>. However,
the neurons used in deep learning and artificial neural networks are
highly abstract, and the architecture is unable to account for the
neurons <span class="citation">(<a href="#ref-cichy_deep_2019">Cichy and
Kaiser 2019</a>)</span>. Therefore, this field has focused more
attention on fitting data rather than interpreting it <span class="citation">(<a href="#ref-pichler_machine_2023">Pichler and Hartig
2023</a>)</span>.</p>
<p>Currently, the field of deep learning is more concerned with fitting
data, the effect of fitting is used as a guiding criterion in this
field, rather than integrating cognitive mechanisms discovered by
neuroscience <span class="citation">(<a href="#ref-chavlis_drawing_2021">Chavlis and Poirazi 2021</a>)</span>.
Much of the progress in deep learning over recent decades can be
attributed to the application of backpropagation, often used with
optimization methods to update weights and minimize the loss function.
However, while neural networks and deep learning are biologically
inspired approaches, the biological rationality of backpropagation
remains questionable, as activated neurons do not acquire features
through backpropagation <span class="citation">(<a href="#ref-whittington_theories_2019">Whittington and Bogacz 2019</a>;
<a href="#ref-lillicrap_backpropagation_2020">Lillicrap et al. 2020</a>;
<a href="#ref-aru_cellular_2020">Aru, Suzuki, and Larkum
2020</a>)</span>. Currently, two mainstream learning mechanisms have
been identified in the human brain using electrophysiological methods:
Hebbian learning <span class="citation">(<a href="#ref-munakata_hebbian_2004">Munakata and Pfaffly 2004</a>)</span>
and reinforcement learning <span class="citation">(<a href="#ref-botvinick_reinforcement_2019">Botvinick et al.
2019</a>)</span>. Additionally, synaptic pruning may be related to
learning <span class="citation">(<a href="#ref-halassa_integrated_2010">Halassa and Haydon 2010</a>)</span>,
and epigenetic mechanisms also play an important role <span class="citation">(<a href="#ref-changeux_theory_1973">Changeux,
Courrège, and Danchin 1973</a>)</span>. Although Hebbian learning,
reinforcement learning, and attempts to migrate human cognitive
mechanisms have been applied in deep learning for years, they still
cannot perfectly reproduce human learning features <span class="citation">(<a href="#ref-volzhenin_multilevel_2022">Volzhenin,
Changeux, and Dumas 2022</a>)</span>. Comparatively, the energy
consumption when using neural networks for reasoning is huge <span class="citation">(<a href="#ref-desislavov_trends_2023">Desislavov,
Martínez-Plumed, and Hernández-Orallo 2023</a>)</span>, in contrast to
the human brain’s lower energy usage for training and reasoning <span class="citation">(<a href="#ref-attwell_energy_2001">Attwell and
Laughlin 2001</a>)</span>.</p>
<p>Another example is the attention mechanism in neural networks,
inspired by human attention <span class="citation">(<a href="#ref-vaswani_attention_2023">Vaswani et al. 2023</a>)</span>.
Attention is a cognitive ability that selectively receives information
with limited resources <span class="citation">(<a href="#ref-cowan_capacity_2005">Nelson Cowan et al. 2005</a>)</span>.
It’s a complex biological process involving multiple brain regions,
encompassing not only selective attention but also coordinated
consciousness, memory, and cognition. Selective attention mechanisms are
associated with short-term memory, where only 3-5 chunks of original
stimuli can enter during a single session <span class="citation">(<a href="#ref-cowan_magical_2001">N. Cowan 2001</a>)</span>, with attention
lasting just a few seconds to minutes <span class="citation">(<a href="#ref-polti_effect_2018">Polti, Martin, and Van Wassenhove
2018</a>)</span>. This selective mechanism allows humans to focus on
targets with limited resources, reducing cognitive resource consumption
<span class="citation">(<a href="#ref-buschman_behavior_2015">Buschman
and Kastner 2015</a>)</span>, refining elements deposited into memory
<span class="citation">(<a href="#ref-chun_interactions_2007">Chun and
Turk-Browne 2007</a>)</span>, delimiting the problem space, and
narrowing memory retrieval in problem-solving situations <span class="citation">(<a href="#ref-wiley_working_2012">Wiley and Jarosz
2012</a>)</span>. Human intuition about numbers may also relate to
attention <span class="citation">(<a href="#ref-kutter_distinct_2023">Kutter et al. 2023</a>)</span>. Thus,
selective attention is crucial for cognitive activities like perception,
memory, and decision-making.</p>
<p>Attention mechanisms in deep learning, inspired by human selective
attention, which have been successfully integrated into various
frameworks <span class="citation">(<a href="#ref-niu_review_2021">Niu,
Zhong, and Yu 2021</a>)</span>, greatly improving performance in tasks
like natural language processing (NLP), computer vision, and speech
recognition <span class="citation">(<a href="#ref-zhang_neural_2020">B.
Zhang, Xiong, and Su 2020</a>; <a href="#ref-guo_attention_2022">Guo et
al. 2022</a>; <a href="#ref-ding_deep_2021">Ding et al.
2021</a>)</span>. In recent years, the Transformer model, relying on
self-attention mechanisms to process data, has demonstrated superior
performance across various tasks <span class="citation">(<a href="#ref-vaswani_attention_2023">Vaswani et al. 2023</a>; <a href="#ref-khan_transformers_2022">Khan et al. 2022</a>)</span>. Its
multi-head attention mechanism performs multiple parallel self-attention
computations with different parameters, allowing the model to capture
information from different subspaces and improving fitting efficiency
and accuracy <span class="citation">(<a href="#ref-liu_multi_head_2021">Liu, Liu, and Han 2021</a>)</span>.
Practically, with the Transformer, neural networks have made significant
progress in areas like NLP and vision tasks.</p>
<p>Attention mechanisms in deep learning are implemented through
mathematical functions that assign weights to different elements of the
input data <span class="citation">(<a href="#ref-niu_review_2021">Niu,
Zhong, and Yu 2021</a>; <a href="#ref-de_santana_correia_attention_2022">De Santana Correia and
Colombini 2022</a>)</span>. However, a subset of studies has found that
the attention mechanism in deep learning cannot fully simulate human
attention and lacks the cognitive and emotional context that human
attention encompasses <span class="citation">(<a href="#ref-lai_understanding_2021">Lai et al. 2021</a>)</span>. Despite
these differences, artificial neural networks have been successfully
applied in several fields, including image and speech recognition,
natural language processing, robot control, gaming, and decision support
systems. These applications demonstrate the power of artificial neural
networks in dealing with complex problems and simulating certain human
cognitive processes while highlighting the unique advantages of models
that simulate human cognitive abilities.</p>
</div>
<div class="section level2">
<h2 id="simulating-human-cognitive-abilities-the-way-forward">3. Simulating Human Cognitive Abilities: The Way Forward<a class="anchor" aria-label="anchor" href="#simulating-human-cognitive-abilities-the-way-forward"></a>
</h2>
<p>In recent years, the Transformer model has excelled in various tasks
that rely on self-attentive mechanisms for data processing <span class="citation">(<a href="#ref-vaswani_attention_2023">Vaswani et al.
2023</a>; <a href="#ref-khan_transformers_2022">Khan et al.
2022</a>)</span>. It departs from traditional recurrent neural networks
(RNNs) and convolutional neural networks (CNNs), favoring a
comprehensive utilization of attentional mechanisms to process
sequential data. The Transformer’s attention model is primarily applied
through self-attention and multi-head attention mechanisms. The
self-attention mechanism considers all other elements in the sequence
when processing each input element, enabling the model to capture
long-range dependencies within the sequence <span class="citation">(<a href="#ref-vig_analyzing_2019">Vig and Belinkov 2019</a>)</span>. Each
element is transformed into query (<em>q</em>), key (<em>k</em>), and
value (<em>v</em>) vectors, representing the current lexical element,
other lexical elements, and the information contained in the lexical
element, respectively. The attention score is computed by calculating
the similarity scores of <em>q</em> and <em>k</em>, and weighted summing
over <em>v</em>. Recently, LLMs have employed the Transformer’s
framework, demonstrating an improved simulation of human cognition.</p>
<p>LLMs are large-scale simulations of human cognitive functions <span class="citation">(<a href="#ref-binz_turning_2023">Binz and Schulz
2023</a>)</span>, and their emergence mark a significant advancement in
computers’ ability to simulate human cognition. LLMs possess enhanced
reasoning capabilities, and Claude 3, released this month by Anthropic,
exhibits self-awareness through contextual understanding in a
needle-in-a-haystack task <span class="citation">(<a href="#ref-anthropic_claude_2024">Anthropic 2024</a>; <a href="#ref-kuratov_search_2024">Kuratov et al. 2024</a>)</span>. In
zero-shot problem scenarios, LLMs’ reasoning abilities without prior
knowledge surpass those of humans, who rely on analogies for reasoning
<span class="citation">(<a href="#ref-webb_emergent_2023">Webb, Holyoak,
and Lu 2023</a>)</span>. Furthermore, LLMs can comprehend others’
beliefs, goals, and mental states with an accuracy of up to 80%.
Notably, GPT-4, considered the most advanced LLM, can achieve 100%
accuracy in theory of mind (ToM) tasks after suitable prompting,
indicating a human-like level of ToM <span class="citation">(<a href="#ref-thaler_anomalies_1988">Thaler 1988</a>)</span>.</p>
<p>LLMs can also simulate human behavior observed in experiments, such
as the ultimatum game <span class="citation">(<a href="#ref-thaler_anomalies_1988">Thaler 1988</a>)</span>, garden-path
sentences <span class="citation">(<a href="#ref-ferreira_misinterpretations_2001">Ferreira, Christianson, and
Hollingworth 2001</a>)</span>, loss aversion <span class="citation">(<a href="#ref-kimball_standard_1993">Kimball 1993</a>)</span>, and
reactions to the Milgram electric shock experiment <span class="citation">(<a href="#ref-blass_milgram_1999">Blass 1999</a>; <a href="#ref-aher_using_2022">Aher, Arriaga, and Kalai 2022</a>)</span>.
Additionally, LLMs exhibit cognitive biases or errors that humans
typically demonstrate, such as additive bias <span class="citation">(<a href="#ref-winter_more_2023">Winter et al. 2023</a>)</span>, where
individuals default to adding or modifying existing content rather than
deleting or pushing back when problem-solving <span class="citation">(<a href="#ref-adams_people_2021">Adams et al. 2021</a>)</span>. LLMs
produce various human cognitive effects, including priming effects and
biases <span class="citation">(<a href="#ref-koo_benchmarking_2023">Koo
et al. 2023</a>; <a href="#ref-shaki_cognitive_2023">Shaki, Kraus, and
Wooldridge 2023</a>)</span>, suggesting that LLMs mimicking human
cognitive processes may possess cognitive abilities approaching the
human level.</p>
<p>In specific domains, LLMs closely mimic human-specific abilities. For
instance, ChatGPT’s accuracy in medical diagnosis and providing feasible
medical advice in complex situations is comparable to that of human
physicians <span class="citation">(<a href="#ref-hopkins_artificial_2023">Hopkins et al. 2023</a>)</span>. The
performance metrics show that it diagnoses up to 93.3% of common
clinical cases correctly <span class="citation">(<a href="#ref-hirosawa_diagnostic_2023">Hirosawa et al. 2023</a>)</span>.
Furthermore, in standardized clinical decision-making tasks, ChatGPT
achieves an accuracy rate close to 70% <span class="citation">(<a href="#ref-rao_assessing_2023">Rao et al. 2023</a>)</span>, similar to
the expected level of third-year medical students in the United States
<span class="citation">(<a href="#ref-gilson_how_2023">Gilson et al.
2023</a>)</span>. Due to GPT-4’s superior ToM, it correctly answered 90%
of soft skill questions <span class="citation">(<a href="#ref-brin_comparing_2023">Brin et al. 2023</a>)</span>,
demonstrating excellent clinical skills.</p>
<p>However, ChatGPT’s ability to handle complex questions remains
unsatisfactory compared to widely used technologies like Google search
<span class="citation">(<a href="#ref-hopkins_artificial_2023">Hopkins
et al. 2023</a>)</span>. It cannot fully replicate professional
clinicians’ decision-making abilities when faced with complex problems,
primarily due to its text-based training data, resulting in less
satisfactory performance in non-text-based tasks <span class="citation">(<a href="#ref-zhang_unexpectedly_2024">Y. Zhang et al.
2024</a>)</span>. Furthermore, in patient care and other medically
related domains, it sometimes generates false or misleading information,
potentially causing doctors, nurses, or caregivers to make erroneous
decisions, endangering patients’ lives <span class="citation">(<a href="#ref-ji_survey_2023">Z. Ji et al. 2023</a>)</span>.</p>
<p>LLMs often contain billions to hundreds of billions of parameters,
making it difficult to implement debugging and understand their
decision-making processes <span class="citation">(<a href="#ref-ji_survey_2023">Z. Ji et al. 2023</a>; <a href="#ref-khullar_large_2024">Khullar, Wang, and Wang 2024</a>)</span>.
Therefore, developing relatively interpretable models is a viable
alternative at the moment. These models are trained in specific areas of
expertise, possessing prior knowledge and learning not exclusively from
samples. Recently, the life2vec model successfully predicted the
relationship between early mortality and aspects of an individual’s
personality traits, demonstrating relatively good predictive efficacy
<span class="citation">(<a href="#ref-savcisens_using_2023">Savcisens et
al. 2023</a>)</span>. The model provides clinicians and family
physicians with insights and assistance that can help patients better
manage their lifespan, showcasing the potential of specialized
models.</p>
</div>
<div class="section level2">
<h2 id="simulating-human-knowledge-representation-using-large-scale-medical-knowledge-networks">4. Simulating human knowledge representation using large-scale
medical knowledge networks<a class="anchor" aria-label="anchor" href="#simulating-human-knowledge-representation-using-large-scale-medical-knowledge-networks"></a>
</h2>
<p>In summary, we have found that computer models simulating human
cognitive abilities tend to achieve very good model fitting results,
such as Transformer-based neural network models like LLMs. While LLMs
perform satisfactorily in a wide range of contexts, there are multiple
aspects that have not been addressed adequately in addition to the
aforementioned medical issues.</p>
<p>LLMs require an enormous number of parameters and a vast amount of
training data, consuming substantial computational resources during the
training process <span class="citation">(<a href="#ref-zhang_opt_2022">S. Zhang et al. 2022</a>)</span>. Even after
training, reasoning with LLMs consumes significant computational
resources <span class="citation">(<a href="#ref-samsi_words_2023">Samsi
et al. 2023</a>)</span>. Furthermore, LLMs produce a large carbon
footprint <span class="citation">(<a href="#ref-zhang_opt_2022">S. Zhang
et al. 2022</a>; <a href="#ref-faiz_llmcarbon_2023">Faiz et al.
2023</a>)</span> and require considerable water consumption for cooling
<span class="citation">(<a href="#ref-george_environmental_2023">George,
A.S.Hovan George, and A.S.Gabrio Martin 2023</a>)</span>, exacerbating
environmental concerns and extreme climate events. As computational
power is concentrated in a few labs, LLM also exacerbates inequality
issues and prevents most labs from gaining LLMs <span class="citation">(<a href="#ref-zhang_opt_2022">S. Zhang et al.
2022</a>)</span>.</p>
<p>LLMs are often considered black boxes, making it difficult to
understand and explain their operating mechanisms. Recently, OpenAI has
demonstrated early forms of artificial intelligence in LLMs by
increasing their parameters and training sample size <span class="citation">(<a href="#ref-openai_gpt_4_2023">OpenAI et al.
2023</a>; <a href="#ref-bubeck_sparks_2023">Bubeck et al. 2023</a>; <a href="#ref-schaeffer_are_2023">Schaeffer, Miranda, and Koyejo 2023</a>;
<a href="#ref-wei_emergent_2022">Wei et al. 2022</a>)</span>,
challenging many scholars’ perceptions. Some have argued that it
resembles the Chinese room problem, where LLMs do not emerge
intelligence but rather acquire deeper features of language, as
consciousness may be a special form of language <span class="citation">(<a href="#ref-hamid_chatgpt_2023">Hamid
2023</a>)</span>. Others contend that the emergent intelligence of LLMs
is merely wishful thinking by researchers <span class="citation">(<a href="#ref-schaeffer_are_2023">Schaeffer, Miranda, and Koyejo
2023</a>)</span>. Alternatively, it has been proposed that LLMs resemble
human societies, where a large number of individuals collectively
exhibit abilities that individuals do not possess, with emergent
capabilities resulting from complex relationships between numerous data
points, akin to ant colony algorithms.</p>
<p>We suspect the possible reasons for their emergence regarding the
capabilities demonstrated by LLMs. As noted earlier, developing
relatively interpretable specialized models would maximize usability and
transparency, making them safer for clinical applications. Over the past
decades, humans have accumulated substantial historical experience in
fighting diseases and a large number of low-practice-value papers and
monographs <span class="citation">(<a href="#ref-hanson_strain_2023">Hanson et al. 2023</a>)</span>.
Translating this experience into clinical resources in bulk has become
an important issue in modern medical research.</p>
<p>We observed that clinicians tend to treat patients automatically
based on their diseases while considering comorbidities the patients may
have, e.g., heart disease, high cholesterol, and bacterial infections.
We aimed to develop a model that could simulate this ability while
maintaining model interpretability. Therefore, we adapted the original
spreading activation model by replacing the LTM with a knowledge network
and substituting the memory search and inference with a random walk
approach to simulate human abilities.</p>
<p>LLMs are often trained using knowledge from publications, and the
promising life2vec model uses medical information from Danish citizens.
Here, we use medical texts to build knowledge networks to train our
models. A knowledge network is a large-scale, graph-structured database
that abstracts core concepts and relationships in reality, allowing AI
systems to understand complex relationships and reason about them. It
can integrate various data sources and types to represent relationships
between elements and their properties. Knowledge networks abstract the
real world for AI systems <span class="citation">(<a href="#ref-martin_modelling_2020">Martin and Baggio 2020</a>)</span>,
enabling them to solve complex tasks and reason about the world <span class="citation">(<a href="#ref-ji_survey_2022">S. Ji et al.
2022</a>)</span>.</p>
<p>Biomedical knowledge is characterized using formalism, an abstraction
process of the human brain to model systems formally and mathematically
<span class="citation">(<a href="#ref-phillips_sheavinguniversal_2020">Phillips 2020</a>)</span>.
Although biomedical knowledge does not use formulas to describe
biological processes like mathematics, physics, and chemistry, knowledge
networks can establish the mechanisms involved in biological processes
<span class="citation">(<a href="#ref-martin_modelling_2020">Martin and
Baggio 2020</a>)</span>. For example, biologists usually use nodes to
represent genes and edges to represent regulatory relationships between
genes.</p>
<p>Once the knowledge network is having constructed, we can simulate how
humans utilize LTM by choosing the random walk approach. Numerous
studies have shown that random walk can effectively simulate human
semantic cognition <span class="citation">(<a href="#ref-ji_survey_2022">S. Ji et al. 2022</a>; <a href="#ref-kumar_semantic_2021">Kumar, Steyvers, and Balota
2021</a>)</span> and is consistent with the human memory retrieval
process. Compared to the outputs of spreading activation, that of
computer-simulated random walks showed higher correlation with the
spreading activation model’s results <span class="citation">(<a href="#ref-abbott_random_2015">Abbott, Austerweil, and Griffiths
2015</a>; <a href="#ref-zemla_estimating_2018">Zemla and Austerweil
2018</a>; <a href="#ref-siew_spreadr_2019">Siew 2019</a>)</span>.
Furthermore, brain scientists have used random walk algorithms to
explore theoretical concepts <span class="citation">(<a href="#ref-abbott_random_2015">Abbott, Austerweil, and Griffiths
2015</a>; <a href="#ref-siew_spreadr_2019">Siew 2019</a>)</span> or
simulate specific human cognitive behaviors to reduce experimental
errors introduced by external environments <span class="citation">(<a href="#ref-abbott_random_2015">Abbott, Austerweil, and Griffiths
2015</a>; <a href="#ref-zemla_estimating_2018">Zemla and Austerweil
2018</a>)</span>.</p>
<p>Similar to the repeated, random selection of various possible
solutions in the human brain, the random walk simulates the random
events that exists in individual problem-solving and decision-making
processes. As a diffusion model, it is applicable to a wide range of
situations, even computer-simulated human societies <span class="citation">(<a href="#ref-park_generative_2023">Park et al.
2023</a>)</span>, demonstrating the broad applicability of such computer
models to many different biological scenarios.</p>
</div>
<div class="section level2">
<h2 id="conclusion">5. Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>Humans excel at employing existing problem-solving strategies <span class="citation">(<a href="#ref-kaula_problem_1995">Kaula
1995</a>)</span>. With the rapid advancement of computer technology,
there has been a surge in research articles on drug repositioning aided
by computational biology and bioinformatics. Figure <span class="math inline">\(\ref{fig:banana}\)</span> demonstrates that the
relative number of articles on drug repositioning included in PubMed,
shows an increasing trend over the years with a more significant rise in
recent years. The calculation method also exhibits the same increasing
trend. The <em>banana</em> metric has proven effective in quantifying
and analyzing research interest trends across various fields, which is
defined as the number of articles retrieved using <em>banana</em> as a
keyword per year <span class="citation">(<a href="#ref-dalmaijer_banana_2021">Dalmaijer et al. 2021</a>)</span>.</p>
<div class="figure">
<img src="img/banana.png" alt="\label{fig:banana}**Bibliometric analysis for drug repurposing. **Drug repurposing gains significant attention since 2010. We adopted banana scale to depict this trend." width="100%"><p class="caption">
<strong>Bibliometric analysis for drug repurposing. </strong>Drug
repurposing gains significant attention since 2010. We adopted banana
scale to depict this trend.
</p>
</div>
<p>We observed that clinicians tend to treat patients symptomatically
based on their diseases while considering other comorbidities the
patients may have, for example, heart disease, hyperlipidemia, and
bacterial infections. We aimed to develop a model that could simulate
this ability while maintaining interpretability. Therefore, we adapted
the original spreading activation theory by replacing the LTM with a
knowledge network and substituting the memory search and inference with
a random walk approach to simulate human abilities.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-abbott_random_2015" class="csl-entry">
Abbott, Joshua T., Joseph L. Austerweil, and Thomas L. Griffiths. 2015.
<span>“Random Walks on Semantic Networks Can Resemble Optimal
Foraging.”</span> <em>Psychological Review</em> 122 (3): 558–69. <a href="https://doi.org/10.1037/a0038693" class="external-link">https://doi.org/10.1037/a0038693</a>.
</div>
<div id="ref-adams_people_2021" class="csl-entry">
Adams, Gabrielle S., Benjamin A. Converse, Andrew H. Hales, and Leidy E.
Klotz. 2021. <span>“People Systematically Overlook Subtractive
Changes.”</span> <em>Nature</em> 592 (7853): 258–61. <a href="https://doi.org/10.1038/s41586-021-03380-y" class="external-link">https://doi.org/10.1038/s41586-021-03380-y</a>.
</div>
<div id="ref-agatonovic_kustrin_basic_2000" class="csl-entry">
Agatonovic-Kustrin, S, and R Beresford. 2000. <span>“Basic Concepts of
Artificial Neural Network (<span>ANN</span>) Modeling and Its
Application in Pharmaceutical Research.”</span> <em>Journal of
Pharmaceutical and Biomedical Analysis</em> 22 (5): 717–27. <a href="https://doi.org/10.1016/S0731-7085(99)00272-1" class="external-link">https://doi.org/10.1016/S0731-7085(99)00272-1</a>.
</div>
<div id="ref-aher_using_2022" class="csl-entry">
Aher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai. 2022. <span>“Using
Large Language Models to Simulate Multiple Humans and Replicate Human
Subject Studies.”</span> <a href="https://doi.org/10.48550/ARXIV.2208.10264" class="external-link">https://doi.org/10.48550/ARXIV.2208.10264</a>.
</div>
<div id="ref-allen_glia_2018" class="csl-entry">
Allen, Nicola J., and David A. Lyons. 2018. <span>“Glia as Architects of
Central Nervous System Formation and Function.”</span> <em>Science (New
York, N.Y.)</em> 362 (6411): 181–85. <a href="https://doi.org/10.1126/science.aat0473" class="external-link">https://doi.org/10.1126/science.aat0473</a>.
</div>
<div id="ref-anderson_spreading_1983" class="csl-entry">
Anderson, John R. 1983. <span>“A Spreading Activation Theory of
Memory.”</span> <em>Journal of Verbal Learning and Verbal Behavior</em>
22 (3): 261–95. <a href="https://doi.org/10.1016/S0022-5371(83)90201-3" class="external-link">https://doi.org/10.1016/S0022-5371(83)90201-3</a>.
</div>
<div id="ref-anthropic_claude_2024" class="csl-entry">
Anthropic. 2024. <span>“The Claude 3 Model Family: Opus, Sonnet,
Haiku.”</span> <a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" class="external-link">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>.
</div>
<div id="ref-aru_cellular_2020" class="csl-entry">
Aru, Jaan, Mototaka Suzuki, and Matthew E. Larkum. 2020. <span>“Cellular
Mechanisms of Conscious Processing.”</span> <em>Trends in Cognitive
Sciences</em> 24 (10): 814–25. <a href="https://doi.org/10.1016/j.tics.2020.07.006" class="external-link">https://doi.org/10.1016/j.tics.2020.07.006</a>.
</div>
<div id="ref-atkinson_human_1968" class="csl-entry">
Atkinson, R. C., and R. M. Shiffrin. 1968. <span>“Human Memory: A
Proposed System and Its Control Processes.”</span> In <em>Psychology of
Learning and Motivation</em>, 2:89–195. Elsevier. <a href="https://doi.org/10.1016/S0079-7421(08)60422-3" class="external-link">https://doi.org/10.1016/S0079-7421(08)60422-3</a>.
</div>
<div id="ref-attwell_energy_2001" class="csl-entry">
Attwell, David, and Simon B. Laughlin. 2001. <span>“An Energy Budget for
Signaling in the Grey Matter of the Brain.”</span> <em>Journal of
Cerebral Blood Flow &amp; Metabolism</em> 21 (10): 1133–45. <a href="https://doi.org/10.1097/00004647-200110000-00001" class="external-link">https://doi.org/10.1097/00004647-200110000-00001</a>.
</div>
<div id="ref-baddeley_episodic_2000" class="csl-entry">
Baddeley, Alan. 2000. <span>“The Episodic Buffer: A New Component of
Working Memory?”</span> <em>Trends in Cognitive Sciences</em> 4 (11):
417–23. <a href="https://doi.org/10.1016/S1364-6613(00)01538-2" class="external-link">https://doi.org/10.1016/S1364-6613(00)01538-2</a>.
</div>
<div id="ref-von_bartheld_search_2016" class="csl-entry">
Bartheld, Christopher S. von, Jami Bahney, and Suzana Herculano-Houzel.
2016. <span>“The Search for True Numbers of Neurons and Glial Cells in
the Human Brain: A Review of 150 Years of Cell Counting.”</span> <em>The
Journal of Comparative Neurology</em> 524 (18): 3865–95. <a href="https://doi.org/10.1002/cne.24040" class="external-link">https://doi.org/10.1002/cne.24040</a>.
</div>
<div id="ref-benjamin_memory_2007" class="csl-entry">
Benjamin, Aaron S. 2007. <span>“Memory Is More Than Just Remembering:
Strategic Control of Encoding, Accessing Memory, and Making
Decisions.”</span> In <em>Psychology of Learning and Motivation</em>,
48:175–223. Elsevier. <a href="https://doi.org/10.1016/S0079-7421(07)48005-7" class="external-link">https://doi.org/10.1016/S0079-7421(07)48005-7</a>.
</div>
<div id="ref-binz_turning_2023" class="csl-entry">
Binz, Marcel, and Eric Schulz. 2023. <span>“Turning Large Language
Models into Cognitive Models.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2306.03917" class="external-link">https://doi.org/10.48550/arXiv.2306.03917</a>.
</div>
<div id="ref-blass_milgram_1999" class="csl-entry">
Blass, Thomas. 1999. <span>“The Milgram Paradigm After 35 Years: Some
Things We Now Know about Obedience to Authority.”</span> <em>Journal of
Applied Social Psychology</em> 29 (5): 955–78. <a href="https://doi.org/10.1111/j.1559-1816.1999.tb00134.x" class="external-link">https://doi.org/10.1111/j.1559-1816.1999.tb00134.x</a>.
</div>
<div id="ref-blumenfeld_lateral_2019" class="csl-entry">
Blumenfeld, Robert S., and Charan Ranganath. 2019. <span>“The Lateral
Prefrontal Cortex and Human Long-Term Memory.”</span> <em>Handbook of
Clinical Neurology</em> 163: 221–35. <a href="https://doi.org/10.1016/B978-0-12-804281-6.00012-4" class="external-link">https://doi.org/10.1016/B978-0-12-804281-6.00012-4</a>.
</div>
<div id="ref-botvinick_reinforcement_2019" class="csl-entry">
Botvinick, Matthew, Sam Ritter, Jane X. Wang, Zeb Kurth-Nelson, Charles
Blundell, and Demis Hassabis. 2019. <span>“Reinforcement Learning, Fast
and Slow.”</span> <em>Trends in Cognitive Sciences</em> 23 (5): 408–22.
<a href="https://doi.org/10.1016/j.tics.2019.02.006" class="external-link">https://doi.org/10.1016/j.tics.2019.02.006</a>.
</div>
<div id="ref-brin_comparing_2023" class="csl-entry">
Brin, Dana, Vera Sorin, Akhil Vaid, Ali Soroush, Benjamin S. Glicksberg,
Alexander W. Charney, Girish Nadkarni, and Eyal Klang. 2023.
<span>“Comparing <span>ChatGPT</span> and <span>GPT</span>-4 Performance
in <span>USMLE</span> Soft Skill Assessments.”</span> <em>Scientific
Reports</em> 13 (1): 16492. <a href="https://doi.org/10.1038/s41598-023-43436-9" class="external-link">https://doi.org/10.1038/s41598-023-43436-9</a>.
</div>
<div id="ref-bubeck_sparks_2023" class="csl-entry">
Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
Eric Horvitz, Ece Kamar, Peter Lee, et al. 2023. <span>“Sparks of
Artificial General Intelligence: Early Experiments with
<span>GPT</span>-4.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2303.12712" class="external-link">https://doi.org/10.48550/arXiv.2303.12712</a>.
</div>
<div id="ref-buschman_behavior_2015" class="csl-entry">
Buschman, Timothy J., and Sabine Kastner. 2015. <span>“From Behavior to
Neural Dynamics: An Integrated Theory of Attention.”</span>
<em>Neuron</em> 88 (1): 127–44. <a href="https://doi.org/10.1016/j.neuron.2015.09.017" class="external-link">https://doi.org/10.1016/j.neuron.2015.09.017</a>.
</div>
<div id="ref-camina_neuroanatomical_2017" class="csl-entry">
Camina, Eduardo, and Francisco Güell. 2017. <span>“The Neuroanatomical,
Neurophysiological and Psychological Basis of Memory: Current Models and
Their Origins.”</span> <em>Frontiers in Pharmacology</em> 8: 438. <a href="https://doi.org/10.3389/fphar.2017.00438" class="external-link">https://doi.org/10.3389/fphar.2017.00438</a>.
</div>
<div id="ref-casasanto_time_2008" class="csl-entry">
Casasanto, Daniel, and Lera Boroditsky. 2008. <span>“Time in the Mind:
Using Space to Think about Time.”</span> <em>Cognition</em> 106 (2):
579–93. <a href="https://doi.org/10.1016/j.cognition.2007.03.004" class="external-link">https://doi.org/10.1016/j.cognition.2007.03.004</a>.
</div>
<div id="ref-changeux_theory_1973" class="csl-entry">
Changeux, J. P., P. Courrège, and A. Danchin. 1973. <span>“A Theory of
the Epigenesis of Neuronal Networks by Selective Stabilization of
Synapses.”</span> <em>Proceedings of the National Academy of Sciences of
the United States of America</em> 70 (10): 2974–78. <a href="https://doi.org/10.1073/pnas.70.10.2974" class="external-link">https://doi.org/10.1073/pnas.70.10.2974</a>.
</div>
<div id="ref-chavlis_drawing_2021" class="csl-entry">
Chavlis, Spyridon, and Panayiota Poirazi. 2021. <span>“Drawing
Inspiration from Biological Dendrites to Empower Artificial Neural
Networks.”</span> <em>Current Opinion in Neurobiology</em> 70 (October):
1–10. <a href="https://doi.org/10.1016/j.conb.2021.04.007" class="external-link">https://doi.org/10.1016/j.conb.2021.04.007</a>.
</div>
<div id="ref-chun_interactions_2007" class="csl-entry">
Chun, Marvin M, and Nicholas B Turk-Browne. 2007. <span>“Interactions
Between Attention and Memory.”</span> <em>Current Opinion in
Neurobiology</em> 17 (2): 177–84. <a href="https://doi.org/10.1016/j.conb.2007.03.005" class="external-link">https://doi.org/10.1016/j.conb.2007.03.005</a>.
</div>
<div id="ref-cichy_deep_2019" class="csl-entry">
Cichy, Radoslaw M., and Daniel Kaiser. 2019. <span>“Deep Neural Networks
as Scientific Models.”</span> <em>Trends in Cognitive Sciences</em> 23
(4): 305–17. <a href="https://doi.org/10.1016/j.tics.2019.01.009" class="external-link">https://doi.org/10.1016/j.tics.2019.01.009</a>.
</div>
<div id="ref-collins_spreading_activation_1975" class="csl-entry">
Collins, Allan M., and Elizabeth F. Loftus. 1975. <span>“A
Spreading-Activation Theory of Semantic Processing.”</span>
<em>Psychological Review</em> 82 (6): 407–28. <a href="https://doi.org/10.1037/0033-295X.82.6.407" class="external-link">https://doi.org/10.1037/0033-295X.82.6.407</a>.
</div>
<div id="ref-cowan_magical_2001" class="csl-entry">
Cowan, N. 2001. <span>“The Magical Number 4 in Short-Term Memory: A
Reconsideration of Mental Storage Capacity.”</span> <em>The Behavioral
and Brain Sciences</em> 24 (1): 87-114; discussion 114-185. <a href="https://doi.org/10.1017/s0140525x01003922" class="external-link">https://doi.org/10.1017/s0140525x01003922</a>.
</div>
<div id="ref-cowan_capacity_2005" class="csl-entry">
Cowan, Nelson, Emily M. Elliott, J. Scott Saults, Candice C. Morey, Sam
Mattox, Anna Hismjatullina, and Andrew R. A. Conway. 2005. <span>“On the
Capacity of Attention: Its Estimation and Its Role in Working Memory and
Cognitive Aptitudes.”</span> <em>Cognitive Psychology</em> 51 (1):
42–100. <a href="https://doi.org/10.1016/j.cogpsych.2004.12.001" class="external-link">https://doi.org/10.1016/j.cogpsych.2004.12.001</a>.
</div>
<div id="ref-dalmaijer_banana_2021" class="csl-entry">
Dalmaijer, Edwin S., Joram Van Rheede, Edwin V. Sperr, and Juliane
Tkotz. 2021. <span>“Banana for Scale: Gauging Trends in Academic
Interest by Normalising Publication Rates to Common and Innocuous
Keywords.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2102.06418" class="external-link">https://doi.org/10.48550/arXiv.2102.06418</a>.
</div>
<div id="ref-de_santana_correia_attention_2022" class="csl-entry">
De Santana Correia, Alana, and Esther Luna Colombini. 2022.
<span>“Attention, Please! A Survey of Neural Attention Models in Deep
Learning.”</span> <em>Artificial Intelligence Review</em> 55 (8):
6037–6124. <a href="https://doi.org/10.1007/s10462-022-10148-x" class="external-link">https://doi.org/10.1007/s10462-022-10148-x</a>.
</div>
<div id="ref-desislavov_trends_2023" class="csl-entry">
Desislavov, Radosvet, Fernando Martínez-Plumed, and José
Hernández-Orallo. 2023. <span>“Trends in <span>AI</span> Inference
Energy Consumption: Beyond the Performance-Vs-Parameter Laws of Deep
Learning.”</span> <em>Sustainable Computing: Informatics and
Systems</em> 38 (April): 100857. <a href="https://doi.org/10.1016/j.suscom.2023.100857" class="external-link">https://doi.org/10.1016/j.suscom.2023.100857</a>.
</div>
<div id="ref-ding_deep_2021" class="csl-entry">
Ding, Huijun, Zixiong Gu, Peng Dai, Zhou Zhou, Lu Wang, and Xiaoxiao Wu.
2021. <span>“Deep Connected Attention (<span>DCA</span>)
<span>ResNet</span> for Robust Voice Pathology Detection and
Classification.”</span> <em>Biomedical Signal Processing and
Control</em> 70 (September): 102973. <a href="https://doi.org/10.1016/j.bspc.2021.102973" class="external-link">https://doi.org/10.1016/j.bspc.2021.102973</a>.
</div>
<div id="ref-faiz_llmcarbon_2023" class="csl-entry">
Faiz, Ahmad, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan
Chen, and Lei Jiang. 2023. <span>“<span>LLMCarbon</span>: Modeling the
End-to-End Carbon Footprint of Large Language Models.”</span> <a href="https://doi.org/10.48550/arXiv.2309.14393" class="external-link">https://doi.org/10.48550/arXiv.2309.14393</a>.
</div>
<div id="ref-ferreira_misinterpretations_2001" class="csl-entry">
Ferreira, Fernanda, Kiel Christianson, and Andrew Hollingworth. 2001.
<span>“Misinterpretations of Garden-Path Sentences: Implications for
Models of Sentence Processing and Reanalysis.”</span> <em>Journal of
Psycholinguistic Research</em> 30 (1): 3–20. <a href="https://doi.org/10.1023/A:1005290706460" class="external-link">https://doi.org/10.1023/A:1005290706460</a>.
</div>
<div id="ref-george_environmental_2023" class="csl-entry">
George, A.Shaji, A.S.Hovan George, and A.S.Gabrio Martin. 2023.
<span>“The Environmental Impact of <span>AI</span>: A Case Study of
Water Consumption by Chat <span>GPT</span>,”</span> April. <a href="https://doi.org/10.5281/ZENODO.7855594" class="external-link">https://doi.org/10.5281/ZENODO.7855594</a>.
</div>
<div id="ref-gilson_how_2023" class="csl-entry">
Gilson, Aidan, Conrad W. Safranek, Thomas Huang, Vimig Socrates, Ling
Chi, Richard Andrew Taylor, and David Chartash. 2023. <span>“How Does
<span>ChatGPT</span> Perform on the United States Medical Licensing
Examination? The Implications of Large Language Models for Medical
Education and Knowledge Assessment.”</span> <em><span>JMIR</span>
Medical Education</em> 9 (February): e45312. <a href="https://doi.org/10.2196/45312" class="external-link">https://doi.org/10.2196/45312</a>.
</div>
<div id="ref-greenwald_measuring_1998" class="csl-entry">
Greenwald, Anthony G., Debbie E. McGhee, and Jordan L. K. Schwartz.
1998. <span>“Measuring Individual Differences in Implicit Cognition: The
Implicit Association Test.”</span> <em>Journal of Personality and Social
Psychology</em> 74 (6): 1464–80. <a href="https://doi.org/10.1037/0022-3514.74.6.1464" class="external-link">https://doi.org/10.1037/0022-3514.74.6.1464</a>.
</div>
<div id="ref-greenwald_understanding_2009" class="csl-entry">
Greenwald, Anthony G., T. Andrew Poehlman, Eric Luis Uhlmann, and
Mahzarin R. Banaji. 2009. <span>“Understanding and Using the Implicit
Association Test: <span>III</span>. Meta-Analysis of Predictive
Validity.”</span> <em>Journal of Personality and Social Psychology</em>
97 (1): 17–41. <a href="https://doi.org/10.1037/a0015575" class="external-link">https://doi.org/10.1037/a0015575</a>.
</div>
<div id="ref-guo_attention_2022" class="csl-entry">
Guo, Meng-Hao, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao
Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph R. Martin, Ming-Ming Cheng,
and Shi-Min Hu. 2022. <span>“Attention Mechanisms in Computer Vision: A
Survey.”</span> <em>Computational Visual Media</em> 8 (3): 331–68. <a href="https://doi.org/10.1007/s41095-022-0271-y" class="external-link">https://doi.org/10.1007/s41095-022-0271-y</a>.
</div>
<div id="ref-halassa_integrated_2010" class="csl-entry">
Halassa, Michael M., and Philip G. Haydon. 2010. <span>“Integrated Brain
Circuits: Astrocytic Networks Modulate Neuronal Activity and
Behavior.”</span> <em>Annual Review of Physiology</em> 72: 335–55. <a href="https://doi.org/10.1146/annurev-physiol-021909-135843" class="external-link">https://doi.org/10.1146/annurev-physiol-021909-135843</a>.
</div>
<div id="ref-hamid_chatgpt_2023" class="csl-entry">
Hamid, Oussama H. 2023. <span>“<span>ChatGPT</span> and the Chinese Room
Argument: An Eloquent <span>AI</span> Conversationalist Lacking True
Understanding and Consciousness.”</span> In <em>2023 9th International
Conference on Information Technology Trends (<span>ITT</span>)</em>,
238–41. Dubai, United Arab Emirates: <span>IEEE</span>. <a href="https://doi.org/10.1109/ITT59889.2023.10184233" class="external-link">https://doi.org/10.1109/ITT59889.2023.10184233</a>.
</div>
<div id="ref-han_rational_2019" class="csl-entry">
Han, Bin, Di Feng, Vincenzo Sciancalepore, and Hans D. Schotten. 2019.
<span>“Rational Impatience Admission Control in 5G-Sliced Networks:
Shall i Bide My Slice Opportunity?”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.1809.06815" class="external-link">https://doi.org/10.48550/arXiv.1809.06815</a>.
</div>
<div id="ref-hanson_strain_2023" class="csl-entry">
Hanson, Mark A., Pablo Gómez Barreiro, Paolo Crosetto, and Dan
Brockington. 2023. <span>“The Strain on Scientific Publishing.”</span>
<span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2309.15884" class="external-link">https://doi.org/10.48550/arXiv.2309.15884</a>.
</div>
<div id="ref-herculano_houzel_human_2009" class="csl-entry">
Herculano-Houzel, Suzana. 2009. <span>“The Human Brain in Numbers: A
Linearly Scaled-up Primate Brain.”</span> <em>Frontiers in Human
Neuroscience</em> 3: 31. <a href="https://doi.org/10.3389/neuro.09.031.2009" class="external-link">https://doi.org/10.3389/neuro.09.031.2009</a>.
</div>
<div id="ref-hirosawa_diagnostic_2023" class="csl-entry">
Hirosawa, Takanobu, Yukinori Harada, Masashi Yokose, Tetsu Sakamoto, Ren
Kawamura, and Taro Shimizu. 2023. <span>“Diagnostic Accuracy of
Differential-Diagnosis Lists Generated by Generative Pretrained
Transformer 3 Chatbot for Clinical Vignettes with Common Chief
Complaints: A Pilot Study.”</span> <em>International Journal of
Environmental Research and Public Health</em> 20 (4): 3378. <a href="https://doi.org/10.3390/ijerph20043378" class="external-link">https://doi.org/10.3390/ijerph20043378</a>.
</div>
<div id="ref-hopkins_artificial_2023" class="csl-entry">
Hopkins, Ashley M, Jessica M Logan, Ganessan Kichenadasse, and Michael J
Sorich. 2023. <span>“Artificial Intelligence Chatbots Will Revolutionize
How Cancer Patients Access Information: <span>ChatGPT</span> Represents
a Paradigm-Shift.”</span> <em><span>JNCI</span> Cancer Spectrum</em> 7
(2): pkad010. <a href="https://doi.org/10.1093/jncics/pkad010" class="external-link">https://doi.org/10.1093/jncics/pkad010</a>.
</div>
<div id="ref-james_stability_2018" class="csl-entry">
James, Lois. 2018. <span>“The Stability of Implicit Racial Bias in
Police Officers.”</span> <em>Police Quarterly</em> 21 (1): 30–52. <a href="https://doi.org/10.1177/1098611117732974" class="external-link">https://doi.org/10.1177/1098611117732974</a>.
</div>
<div id="ref-ji_survey_2022" class="csl-entry">
Ji, Shaoxiong, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S.
Yu. 2022. <span>“A Survey on Knowledge Graphs: Representation,
Acquisition, and Applications.”</span> <em><span>IEEE</span>
Transactions on Neural Networks and Learning Systems</em> 33 (2):
494–514. <a href="https://doi.org/10.1109/TNNLS.2021.3070843" class="external-link">https://doi.org/10.1109/TNNLS.2021.3070843</a>.
</div>
<div id="ref-ji_survey_2023" class="csl-entry">
Ji, Ziwei, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko
Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.
<span>“Survey of Hallucination in Natural Language Generation.”</span>
<em><span>ACM</span> Computing Surveys</em> 55 (12): 1–38. <a href="https://doi.org/10.1145/3571730" class="external-link">https://doi.org/10.1145/3571730</a>.
</div>
<div id="ref-karpinski_attitude_2005" class="csl-entry">
Karpinski, Andrew, Ross B. Steinman, and James L. Hilton. 2005.
<span>“Attitude Importance as a Moderator of the Relationship Between
Implicit and Explicit Attitude Measures.”</span> <em>Personality and
Social Psychology Bulletin</em> 31 (7): 949–62. <a href="https://doi.org/10.1177/0146167204273007" class="external-link">https://doi.org/10.1177/0146167204273007</a>.
</div>
<div id="ref-kaula_problem_1995" class="csl-entry">
Kaula, Rajeev. 1995. <span>“Problem Solving Strategies for Open
Information Systems.”</span> <em>Knowledge-Based Systems</em> 8 (5):
235–48. <a href="https://doi.org/10.1016/0950-7051(95)98901-H" class="external-link">https://doi.org/10.1016/0950-7051(95)98901-H</a>.
</div>
<div id="ref-kazanas_survival_2015" class="csl-entry">
Kazanas, Stephanie A., and Jeanette Altarriba. 2015. <span>“The Survival
Advantage: Underlying Mechanisms and Extant Limitations.”</span>
<em>Evolutionary Psychology</em> 13 (2): 147470491501300. <a href="https://doi.org/10.1177/147470491501300204" class="external-link">https://doi.org/10.1177/147470491501300204</a>.
</div>
<div id="ref-khan_transformers_2022" class="csl-entry">
Khan, Salman, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad
Shahbaz Khan, and Mubarak Shah. 2022. <span>“Transformers in Vision: A
Survey.”</span> <em><span>ACM</span> Computing Surveys</em> 54 (10):
1–41. <a href="https://doi.org/10.1145/3505244" class="external-link">https://doi.org/10.1145/3505244</a>.
</div>
<div id="ref-khullar_large_2024" class="csl-entry">
Khullar, Dhruv, Xingbo Wang, and Fei Wang. 2024. <span>“Large Language
Models in Health Care: Charting a Path Toward Accurate, Explainable, and
Secure <span>AI</span>.”</span> <em>Journal of General Internal
Medicine</em>, February, s11606-024-08657-2. <a href="https://doi.org/10.1007/s11606-024-08657-2" class="external-link">https://doi.org/10.1007/s11606-024-08657-2</a>.
</div>
<div id="ref-kim_neuron_glia_2020" class="csl-entry">
Kim, Yoo Sung, Juwon Choi, and Bo-Eun Yoon. 2020. <span>“Neuron-Glia
Interactions in Neurodevelopmental Disorders.”</span> <em>Cells</em> 9
(10): 2176. <a href="https://doi.org/10.3390/cells9102176" class="external-link">https://doi.org/10.3390/cells9102176</a>.
</div>
<div id="ref-kimball_standard_1993" class="csl-entry">
Kimball, Miles S. 1993. <span>“Standard Risk Aversion.”</span>
<em>Econometrica</em> 61 (3): 589. <a href="https://doi.org/10.2307/2951719" class="external-link">https://doi.org/10.2307/2951719</a>.
</div>
<div id="ref-koo_benchmarking_2023" class="csl-entry">
Koo, Ryan, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and
Dongyeop Kang. 2023. <span>“Benchmarking Cognitive Biases in Large
Language Models as Evaluators.”</span> <a href="https://doi.org/10.48550/arXiv.2309.17012" class="external-link">https://doi.org/10.48550/arXiv.2309.17012</a>.
</div>
<div id="ref-kumar_semantic_2021" class="csl-entry">
Kumar, Abhilasha A., Mark Steyvers, and David A. Balota. 2021.
<span>“Semantic Memory Search and Retrieval in a Novel Cooperative Word
Game: A Comparison of Associative and Distributional Semantic
Models.”</span> <em>Cognitive Science</em> 45 (10): e13053. <a href="https://doi.org/10.1111/cogs.13053" class="external-link">https://doi.org/10.1111/cogs.13053</a>.
</div>
<div id="ref-kuratov_search_2024" class="csl-entry">
Kuratov, Yuri, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom
Sorokin, and Mikhail Burtsev. 2024. <span>“In Search of Needles in a 11M
Haystack: Recurrent Memory Finds What <span>LLMs</span> Miss.”</span>
<span>arXiv</span>. <a href="http://arxiv.org/abs/2402.10790" class="external-link">http://arxiv.org/abs/2402.10790</a>.
</div>
<div id="ref-kutter_distinct_2023" class="csl-entry">
Kutter, Esther F., Gert Dehnen, Valeri Borger, Rainer Surges, Florian
Mormann, and Andreas Nieder. 2023. <span>“Distinct Neuronal
Representation of Small and Large Numbers in the Human Medial Temporal
Lobe.”</span> <em>Nature Human Behaviour</em> 7 (11): 1998–2007. <a href="https://doi.org/10.1038/s41562-023-01709-3" class="external-link">https://doi.org/10.1038/s41562-023-01709-3</a>.
</div>
<div id="ref-lai_understanding_2021" class="csl-entry">
Lai, Qiuxia, Salman Khan, Yongwei Nie, Hanqiu Sun, Jianbing Shen, and
Ling Shao. 2021. <span>“Understanding More about Human and Machine
Attention in Deep Neural Networks.”</span> <em><span>IEEE</span>
Transactions on Multimedia</em> 23: 2086–99. <a href="https://doi.org/10.1109/TMM.2020.3007321" class="external-link">https://doi.org/10.1109/TMM.2020.3007321</a>.
</div>
<div id="ref-lawlor_mendelian_2008" class="csl-entry">
Lawlor, Debbie A., Roger M. Harbord, Jonathan A. C. Sterne, Nic Timpson,
and George Davey Smith. 2008. <span>“Mendelian Randomization: Using
Genes as Instruments for Making Causal Inferences in
Epidemiology.”</span> <em>Statistics in Medicine</em> 27 (8): 1133–63.
<a href="https://doi.org/10.1002/sim.3034" class="external-link">https://doi.org/10.1002/sim.3034</a>.
</div>
<div id="ref-lee_comparison_2013" class="csl-entry">
Lee, Kwang-Ho, and Dae-Young Kim. 2013. <span>“A Comparison of Implicit
and Explicit Attitude Measures: An Application of the Implicit
Association Test (<span>IAT</span>) to Fast Food Restaurant
Brands.”</span> <em>Tourism Analysis</em> 18 (2): 119–31. <a href="https://doi.org/10.3727/108354213X13645733247576" class="external-link">https://doi.org/10.3727/108354213X13645733247576</a>.
</div>
<div id="ref-lehmann_semantic_1992" class="csl-entry">
Lehmann, Fritz. 1992. <span>“Semantic Networks.”</span> <em>Computers
&amp; Mathematics with Applications</em> 23 (2): 1–50. <a href="https://doi.org/10.1016/0898-1221(92)90135-5" class="external-link">https://doi.org/10.1016/0898-1221(92)90135-5</a>.
</div>
<div id="ref-lillicrap_backpropagation_2020" class="csl-entry">
Lillicrap, Timothy P., Adam Santoro, Luke Marris, Colin J. Akerman, and
Geoffrey Hinton. 2020. <span>“Backpropagation and the Brain.”</span>
<em>Nature Reviews. Neuroscience</em> 21 (6): 335–46. <a href="https://doi.org/10.1038/s41583-020-0277-3" class="external-link">https://doi.org/10.1038/s41583-020-0277-3</a>.
</div>
<div id="ref-liu_multi_head_2021" class="csl-entry">
Liu, Liyuan, Jialu Liu, and Jiawei Han. 2021. <span>“Multi-Head or
Single-Head? An Empirical Comparison for Transformer Training.”</span>
<span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2106.09650" class="external-link">https://doi.org/10.48550/arXiv.2106.09650</a>.
</div>
<div id="ref-martin_modelling_2020" class="csl-entry">
Martin, Andrea E., and Giosuè Baggio. 2020. <span>“Modelling Meaning
Composition from Formalism to Mechanism.”</span> <em>Philosophical
Transactions of the Royal Society B: Biological Sciences</em> 375
(1791): 20190298. <a href="https://doi.org/10.1098/rstb.2019.0298" class="external-link">https://doi.org/10.1098/rstb.2019.0298</a>.
</div>
<div id="ref-mayberry_neurolinguistic_2018" class="csl-entry">
Mayberry, Rachel I., Tristan Davenport, Austin Roth, and Eric Halgren.
2018. <span>“Neurolinguistic Processing When the Brain Matures Without
Language.”</span> <em>Cortex</em> 99 (February): 390–403. <a href="https://doi.org/10.1016/j.cortex.2017.12.011" class="external-link">https://doi.org/10.1016/j.cortex.2017.12.011</a>.
</div>
<div id="ref-mhatre_homogeneous_2004" class="csl-entry">
Mhatre, V., and C. Rosenberg. 2004. <span>“Homogeneous Vs Heterogeneous
Clustered Sensor Networks: A Comparative Study.”</span> In <em>2004
<span>IEEE</span> International Conference on Communications
(<span>IEEE</span> Cat. No.04CH37577)</em>, 3646–3651 Vol.6. Paris,
France: <span>IEEE</span>. <a href="https://doi.org/10.1109/ICC.2004.1313223" class="external-link">https://doi.org/10.1109/ICC.2004.1313223</a>.
</div>
<div id="ref-munakata_hebbian_2004" class="csl-entry">
Munakata, Yuko, and Jason Pfaffly. 2004. <span>“Hebbian Learning and
Development.”</span> <em>Developmental Science</em> 7 (2): 141–48. <a href="https://doi.org/10.1111/j.1467-7687.2004.00331.x" class="external-link">https://doi.org/10.1111/j.1467-7687.2004.00331.x</a>.
</div>
<div id="ref-nairne_adaptive_2016" class="csl-entry">
Nairne, James S., and Josefa N. S. Pandeirada. 2016. <span>“Adaptive
Memory: The Evolutionary Significance of Survival Processing.”</span>
<em>Perspectives on Psychological Science</em> 11 (4): 496–511. <a href="https://doi.org/10.1177/1745691616635613" class="external-link">https://doi.org/10.1177/1745691616635613</a>.
</div>
<div id="ref-niu_review_2021" class="csl-entry">
Niu, Zhaoyang, Guoqiang Zhong, and Hui Yu. 2021. <span>“A Review on the
Attention Mechanism of Deep Learning.”</span> <em>Neurocomputing</em>
452 (September): 48–62. <a href="https://doi.org/10.1016/j.neucom.2021.03.091" class="external-link">https://doi.org/10.1016/j.neucom.2021.03.091</a>.
</div>
<div id="ref-nussenbaum_memorys_2020" class="csl-entry">
Nussenbaum, Kate, Euan Prentis, and Catherine A. Hartley. 2020.
<span>“Memory’s Reflection of Learned Information Value Increases Across
Development.”</span> <em>Journal of Experimental Psychology:
General</em> 149 (10): 1919–34. <a href="https://doi.org/10.1037/xge0000753" class="external-link">https://doi.org/10.1037/xge0000753</a>.
</div>
<div id="ref-openai_gpt_4_2023" class="csl-entry">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
Akkaya, Florencia Leoni Aleman, et al. 2023. <span>“<span>GPT</span>-4
Technical Report.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2303.08774" class="external-link">https://doi.org/10.48550/arXiv.2303.08774</a>.
</div>
<div id="ref-pareti_all_or_none_2007" class="csl-entry">
Pareti, G. 2007. <span>“The "All-or-None" Law in Skeletal Muscle and
Nerve Fibres.”</span> <em>Archives Italiennes De Biologie</em> 145 (1):
39–54. <a href="https://doi.org/10.4449/AIB.V145I1.865" class="external-link">https://doi.org/10.4449/AIB.V145I1.865</a>.
</div>
<div id="ref-parisi_artificial_1997" class="csl-entry">
Parisi, Domenico. 1997. <span>“Artificial Life and Higher Level
Cognition.”</span> <em>Brain and Cognition</em> 34 (1): 160–84. <a href="https://doi.org/10.1006/brcg.1997.0911" class="external-link">https://doi.org/10.1006/brcg.1997.0911</a>.
</div>
<div id="ref-park_generative_2023" class="csl-entry">
Park, Joon Sung, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
Morris, Percy Liang, and Michael S. Bernstein. 2023. <span>“Generative
Agents: Interactive Simulacra of Human Behavior.”</span>
<span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2304.03442" class="external-link">https://doi.org/10.48550/arXiv.2304.03442</a>.
</div>
<div id="ref-phillips_sheavinguniversal_2020" class="csl-entry">
Phillips, Steven. 2020. <span>“Sheaving—a Universal Construction for
Semantic Compositionality.”</span> <em>Philosophical Transactions of the
Royal Society B: Biological Sciences</em> 375 (1791): 20190303. <a href="https://doi.org/10.1098/rstb.2019.0303" class="external-link">https://doi.org/10.1098/rstb.2019.0303</a>.
</div>
<div id="ref-pichler_machine_2023" class="csl-entry">
Pichler, Maximilian, and Florian Hartig. 2023. <span>“Machine Learning
and Deep Learning—a Review for Ecologists.”</span> <em>Methods in
Ecology and Evolution</em> 14 (4): 994–1016. <a href="https://doi.org/10.1111/2041-210X.14061" class="external-link">https://doi.org/10.1111/2041-210X.14061</a>.
</div>
<div id="ref-polti_effect_2018" class="csl-entry">
Polti, Ignacio, Benoît Martin, and Virginie Van Wassenhove. 2018.
<span>“The Effect of Attention and Working Memory on the Estimation of
Elapsed Time.”</span> <em>Scientific Reports</em> 8 (1): 6690. <a href="https://doi.org/10.1038/s41598-018-25119-y" class="external-link">https://doi.org/10.1038/s41598-018-25119-y</a>.
</div>
<div id="ref-rao_assessing_2023" class="csl-entry">
Rao, Arya, Michael Pang, John Kim, Meghana Kamineni, Winston Lie, Anoop
K. Prasad, Adam Landman, Keith J. Dreyer, and Marc D. Succi. 2023.
<span>“Assessing the Utility of <span>ChatGPT</span> Throughout the
Entire Clinical Workflow.”</span> <em><span>medRxiv</span>: The Preprint
Server for Health Sciences</em>, February, 2023.02.21.23285886. <a href="https://doi.org/10.1101/2023.02.21.23285886" class="external-link">https://doi.org/10.1101/2023.02.21.23285886</a>.
</div>
<div id="ref-renoult_knowing_2019" class="csl-entry">
Renoult, Louis, Muireann Irish, Morris Moscovitch, and Michael D. Rugg.
2019. <span>“From Knowing to Remembering: The Semantic-Episodic
Distinction.”</span> <em>Trends in Cognitive Sciences</em> 23 (12):
1041–57. <a href="https://doi.org/10.1016/j.tics.2019.09.008" class="external-link">https://doi.org/10.1016/j.tics.2019.09.008</a>.
</div>
<div id="ref-samsi_words_2023" class="csl-entry">
Samsi, Siddharth, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas,
Michael Jones, William Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay
Gadepally. 2023. <span>“From Words to Watts: Benchmarking the Energy
Costs of Large Language Model Inference.”</span> In <em>2023
<span>IEEE</span> High Performance Extreme Computing Conference
(<span>HPEC</span>)</em>, 1–9. Boston, <span>MA</span>,
<span>USA</span>: <span>IEEE</span>. <a href="https://doi.org/10.1109/HPEC58863.2023.10363447" class="external-link">https://doi.org/10.1109/HPEC58863.2023.10363447</a>.
</div>
<div id="ref-savcisens_using_2023" class="csl-entry">
Savcisens, Germans, Tina Eliassi-Rad, Lars Kai Hansen, Laust Hvas
Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, and Sune Lehmann.
2023. <span>“Using Sequences of Life-Events to Predict Human
Lives.”</span> <em>Nature Computational Science</em> 4 (1): 43–56. <a href="https://doi.org/10.1038/s43588-023-00573-5" class="external-link">https://doi.org/10.1038/s43588-023-00573-5</a>.
</div>
<div id="ref-schacter_future_2012" class="csl-entry">
Schacter, Daniel L., Donna Rose Addis, Demis Hassabis, Victoria C.
Martin, R. Nathan Spreng, and Karl K. Szpunar. 2012. <span>“The Future
of Memory: Remembering, Imagining, and the Brain.”</span>
<em>Neuron</em> 76 (4): 677–94. <a href="https://doi.org/10.1016/j.neuron.2012.11.001" class="external-link">https://doi.org/10.1016/j.neuron.2012.11.001</a>.
</div>
<div id="ref-schaeffer_are_2023" class="csl-entry">
Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo. 2023. <span>“Are
Emergent Abilities of Large Language Models a Mirage?”</span>
<span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2304.15004" class="external-link">https://doi.org/10.48550/arXiv.2304.15004</a>.
</div>
<div id="ref-shaki_cognitive_2023" class="csl-entry">
Shaki, Jonathan, Sarit Kraus, and Michael Wooldridge. 2023.
<span>“Cognitive Effects in Large Language Models.”</span> <a href="https://doi.org/10.48550/ARXIV.2308.14337" class="external-link">https://doi.org/10.48550/ARXIV.2308.14337</a>.
</div>
<div id="ref-sharifian_hierarchical_1997" class="csl-entry">
Sharifian, Farzad, and Ramin Samani. 1997. <span>“Hierarchical Spreading
of Activation.”</span> In <em>Proc. Of the Conference on Language,
Cognition, and Interpretation</em>, 1–10. <span>IAU</span> Press
Isfahan.
</div>
<div id="ref-siew_spreadr_2019" class="csl-entry">
Siew, Cynthia S. Q. 2019. <span>“Spreadr: An r Package to Simulate
Spreading Activation in a Network.”</span> <em>Behavior Research
Methods</em> 51 (2): 910–29. <a href="https://doi.org/10.3758/s13428-018-1186-5" class="external-link">https://doi.org/10.3758/s13428-018-1186-5</a>.
</div>
<div id="ref-simmons_anterior_2009" class="csl-entry">
Simmons, W. Kyle, and Alex Martin. 2009. <span>“The Anterior Temporal
Lobes and the Functional Architecture of Semantic Memory.”</span>
<em>Journal of the International Neuropsychological Society:
<span>JINS</span></em> 15 (5): 645–49. <a href="https://doi.org/10.1017/S1355617709990348" class="external-link">https://doi.org/10.1017/S1355617709990348</a>.
</div>
<div id="ref-smith_multiple_2008" class="csl-entry">
Smith, Edward E., and Murray Grossman. 2008. <span>“Multiple Systems of
Category Learning.”</span> <em>Neuroscience and Biobehavioral
Reviews</em> 32 (2): 249–64. <a href="https://doi.org/10.1016/j.neubiorev.2007.07.009" class="external-link">https://doi.org/10.1016/j.neubiorev.2007.07.009</a>.
</div>
<div id="ref-squire_medial_1991" class="csl-entry">
Squire, Larry R., and Stuart Zola-Morgan. 1991. <span>“The Medial
Temporal Lobe Memory System.”</span> <em>Science</em> 253 (5026):
1380–86. <a href="https://doi.org/10.1126/science.1896849" class="external-link">https://doi.org/10.1126/science.1896849</a>.
</div>
<div id="ref-thaler_anomalies_1988" class="csl-entry">
Thaler, Richard H. 1988. <span>“Anomalies: The Ultimatum Game.”</span>
<em>Journal of Economic Perspectives</em> 2 (4): 195–206. <a href="https://doi.org/10.1257/jep.2.4.195" class="external-link">https://doi.org/10.1257/jep.2.4.195</a>.
</div>
<div id="ref-vaswani_attention_2023" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.
<span>“Attention Is All You Need.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.1706.03762" class="external-link">https://doi.org/10.48550/arXiv.1706.03762</a>.
</div>
<div id="ref-vig_analyzing_2019" class="csl-entry">
Vig, Jesse, and Yonatan Belinkov. 2019. <span>“Analyzing the Structure
of Attention in a Transformer Language Model.”</span> In <em>Proceedings
of the 2019 <span>ACL</span> Workshop <span>BlackboxNLP</span>:
Analyzing and Interpreting Neural Networks for <span>NLP</span></em>,
63–76. Florence, Italy: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/W19-4808" class="external-link">https://doi.org/10.18653/v1/W19-4808</a>.
</div>
<div id="ref-volzhenin_multilevel_2022" class="csl-entry">
Volzhenin, Konstantin, Jean-Pierre Changeux, and Guillaume Dumas. 2022.
<span>“Multilevel Development of Cognitive Abilities in an Artificial
Neural Network.”</span> <em>Proceedings of the National Academy of
Sciences</em> 119 (39): e2201304119. <a href="https://doi.org/10.1073/pnas.2201304119" class="external-link">https://doi.org/10.1073/pnas.2201304119</a>.
</div>
<div id="ref-webb_emergent_2023" class="csl-entry">
Webb, Taylor, Keith J. Holyoak, and Hongjing Lu. 2023. <span>“Emergent
Analogical Reasoning in Large Language Models.”</span> <em>Nature Human
Behaviour</em> 7 (9): 1526–41. <a href="https://doi.org/10.1038/s41562-023-01659-w" class="external-link">https://doi.org/10.1038/s41562-023-01659-w</a>.
</div>
<div id="ref-wei_emergent_2022" class="csl-entry">
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
Sebastian Borgeaud, Dani Yogatama, et al. 2022. <span>“Emergent
Abilities of Large Language Models.”</span> <span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2206.07682" class="external-link">https://doi.org/10.48550/arXiv.2206.07682</a>.
</div>
<div id="ref-whittington_theories_2019" class="csl-entry">
Whittington, James C. R., and Rafal Bogacz. 2019. <span>“Theories of
Error Back-Propagation in the Brain.”</span> <em>Trends in Cognitive
Sciences</em> 23 (3): 235–50. <a href="https://doi.org/10.1016/j.tics.2018.12.005" class="external-link">https://doi.org/10.1016/j.tics.2018.12.005</a>.
</div>
<div id="ref-wiley_working_2012" class="csl-entry">
Wiley, Jennifer, and Andrew F. Jarosz. 2012. <span>“Working Memory
Capacity, Attentional Focus, and Problem Solving.”</span> <em>Current
Directions in Psychological Science</em> 21 (4): 258–62. <a href="https://doi.org/10.1177/0963721412447622" class="external-link">https://doi.org/10.1177/0963721412447622</a>.
</div>
<div id="ref-winter_more_2023" class="csl-entry">
Winter, Bodo, Martin H. Fischer, Christoph Scheepers, and Andriy
Myachykov. 2023. <span>“More Is Better: English Language Statistics Are
Biased Toward Addition.”</span> <em>Cognitive Science</em> 47 (4):
e13254. <a href="https://doi.org/10.1111/cogs.13254" class="external-link">https://doi.org/10.1111/cogs.13254</a>.
</div>
<div id="ref-wolosker_d_amino_2008" class="csl-entry">
Wolosker, Herman, Elena Dumin, Livia Balan, and Veronika N. Foltyn.
2008. <span>“D-Amino Acids in the Brain: D-Serine in Neurotransmission
and Neurodegeneration.”</span> <em>The <span>FEBS</span> Journal</em>
275 (14): 3514–26. <a href="https://doi.org/10.1111/j.1742-4658.2008.06515.x" class="external-link">https://doi.org/10.1111/j.1742-4658.2008.06515.x</a>.
</div>
<div id="ref-yang_neuroinflammation_2019" class="csl-entry">
Yang, Qiao-Qiao, and Jia-Wei Zhou. 2019. <span>“Neuroinflammation in the
Central Nervous System: Symphony of Glial Cells.”</span> <em>Glia</em>
67 (6): 1017–35. <a href="https://doi.org/10.1002/glia.23571" class="external-link">https://doi.org/10.1002/glia.23571</a>.
</div>
<div id="ref-zahedi_introduction_1991" class="csl-entry">
Zahedi, Fatemeh. 1991. <span>“An Introduction to Neural Networks and a
Comparison with Artificial Intelligence and Expert Systems.”</span>
<em>Interfaces</em> 21 (2): 25–38. <a href="https://doi.org/10.1287/inte.21.2.25" class="external-link">https://doi.org/10.1287/inte.21.2.25</a>.
</div>
<div id="ref-zemla_estimating_2018" class="csl-entry">
Zemla, Jeffrey C., and Joseph L. Austerweil. 2018. <span>“Estimating
Semantic Networks of Groups and Individuals from Fluency Data.”</span>
<em>Computational Brain &amp; Behavior</em> 1 (1): 36–58. <a href="https://doi.org/10.1007/s42113-018-0003-7" class="external-link">https://doi.org/10.1007/s42113-018-0003-7</a>.
</div>
<div id="ref-zhang_neural_2020" class="csl-entry">
Zhang, Biao, Deyi Xiong, and Jinsong Su. 2020. <span>“Neural Machine
Translation with Deep Attention.”</span> <em><span>IEEE</span>
Transactions on Pattern Analysis and Machine Intelligence</em> 42 (1):
154–63. <a href="https://doi.org/10.1109/TPAMI.2018.2876404" class="external-link">https://doi.org/10.1109/TPAMI.2018.2876404</a>.
</div>
<div id="ref-zhang_algorithm_2023" class="csl-entry">
Zhang, He, Liang Zhang, Ang Lin, Congcong Xu, Ziyu Li, Kaibo Liu,
Boxiang Liu, et al. 2023. <span>“Algorithm for Optimized <span class="nocase">mRNA</span> Design Improves Stability and
Immunogenicity.”</span> <em>Nature</em> 621 (7978): 396–403. <a href="https://doi.org/10.1038/s41586-023-06127-z" class="external-link">https://doi.org/10.1038/s41586-023-06127-z</a>.
</div>
<div id="ref-zhang_opt_2022" class="csl-entry">
Zhang, Susan, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,
Shuohui Chen, Christopher Dewan, et al. 2022. <span>“<span>OPT</span>:
Open Pre-Trained Transformer Language Models.”</span>
<span>arXiv</span>. <a href="https://doi.org/10.48550/arXiv.2205.01068" class="external-link">https://doi.org/10.48550/arXiv.2205.01068</a>.
</div>
<div id="ref-zhang_unexpectedly_2024" class="csl-entry">
Zhang, Yiwen, Liwei Wu, Yangang Wang, Bin Sheng, Yih Chung Tham, Hongwei
Ji, Ying Chen, Linlin Ren, Hanyun Liu, and Lili Xu. 2024.
<span>“Unexpectedly Low Accuracy of <span>GPT</span>-4 in Identifying
Common Liver Diseases from <span>CT</span> Scan Images.”</span>
<em>Digestive and Liver Disease</em>, February, S1590865824002111. <a href="https://doi.org/10.1016/j.dld.2024.01.191" class="external-link">https://doi.org/10.1016/j.dld.2024.01.191</a>.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Junwei Han, Yinchun Su.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.9.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
